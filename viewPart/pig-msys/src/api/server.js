/**
 * é›†æˆåœ¨ Vue ä¸­çš„ Node.js åç«¯æœåŠ¡
 * åœ¨ main.js ä¸­å¯åŠ¨ï¼Œå¤„ç† Hadoop MapReduce ä»»åŠ¡æäº¤
 */
const express = require('express');
const cors = require('cors');
const { exec } = require('child_process');

const app = express();
const PORT = process.env.HADOOP_BRIDGE_PORT || 3100;

// ä¸­é—´ä»¶
app.use(cors());
app.use(express.json());

// CORS preflight è¯·æ±‚å¤„ç†
app.options('*', cors());

// è¯·æ±‚æ—¥å¿—
app.use((req, res, next) => {
  console.log(`ğŸ“¨ [${new Date().toISOString()}] ${req.method} ${req.path}`);
  next();
});

/**
 * æ ¹è·¯ç”±ï¼ˆå¥åº·æ£€æŸ¥ï¼‰
 */
app.get('/', (req, res) => {
  res.json({
    status: 'ok',
    service: 'hadoop-bridge-running',
    timestamp: new Date().toISOString(),
    port: PORT
  });
});

/**
 * å¥åº·æ£€æŸ¥
 */
app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    service: 'hadoop-bridge',
    timestamp: new Date().toISOString()
  });
});

/**
 * æäº¤ç¯å¢ƒæ•°æ®åˆ†æä»»åŠ¡
 */
app.post('/api/hadoop/submit-environment-analysis', (req, res) => {
  try {
    console.log('âœ… æ”¶åˆ°ç¯å¢ƒåˆ†æä»»åŠ¡è¯·æ±‚');
    const HADOOP_HOME = process.env.HADOOP_HOME || '/opt/homebrew/opt/hadoop/libexec';
    const jobName = 'pig-environment-analysis';
    const inputPath = '/pig-system/dashboard';  // ä¿®æ”¹ä¸ºå®é™…å­˜åœ¨çš„è·¯å¾„
    const outputPath = `/pig-system/output/env_analysis_${Date.now()}`;  // è¾“å‡ºåˆ° output ç›®å½•

    console.log(`ğŸš€ [Hadoop Bridge] æ­£åœ¨æäº¤ MapReduce ä»»åŠ¡: ${jobName}`);
    console.log(`   ğŸ“‚ è¾“å…¥: ${inputPath}`);
    console.log(`   ğŸ“‚ è¾“å‡º: ${outputPath}`);

    // å¼‚æ­¥æ‰§è¡Œï¼ˆfire-and-forgetï¼‰
    const command = `
      export HADOOP_HOME=${HADOOP_HOME} && \
      JAR_PATH=$(ls ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar 2>/dev/null | head -n1) && \
      if [ -z "$JAR_PATH" ]; then
        echo "é”™è¯¯: æ‰¾ä¸åˆ° hadoop-streaming jar" >&2
        exit 1
      fi && \
      hadoop jar "$JAR_PATH" \
        -D mapreduce.job.name="${jobName}" \
        -input "${inputPath}" \
        -output "${outputPath}" \
        -mapper "cat" \
        -reducer "wc -l"
    `.trim();

    const child = exec(command, {
      env: { ...process.env, HADOOP_HOME },
      maxBuffer: 1024 * 1024 * 10,
      shell: '/bin/bash'
    });

    const pid = child.pid;
    console.log(`âœ… ä»»åŠ¡å·²æäº¤ï¼ŒPID: ${pid}`);

    // ç«‹å³è¿”å›
    console.log('âœ… è¿”å›å“åº”ç»™å‰ç«¯');
    res.json({
      code: 200,
      message: 'ç¯å¢ƒåˆ†æä»»åŠ¡å·²æäº¤',
      data: {
        pid,
        jobName,
        submittedAt: new Date().toISOString()
      }
    });

    // ç›‘å¬è¾“å‡º
    child.stdout.on('data', (data) => {
      console.log(`[Hadoop STDOUT]: ${data.toString().trim()}`);
    });

    child.stderr.on('data', (data) => {
      console.error(`[Hadoop STDERR]: ${data.toString().trim()}`);
    });

    child.on('exit', (code) => {
      code === 0 
        ? console.log(`âœ… ä»»åŠ¡å®Œæˆ (PID: ${pid})`)
        : console.error(`âŒ ä»»åŠ¡å¤±è´¥ (PID: ${pid}, é€€å‡ºç : ${code})`);
    });

    // ç«‹å³è¿”å›
    res.json({
      code: 200,
      message: 'ç¯å¢ƒåˆ†æä»»åŠ¡å·²æäº¤',
      data: {
        pid,
        jobName,
        submittedAt: new Date().toISOString()
      }
    });
  } catch (error) {
    console.error('âŒ æäº¤ä»»åŠ¡å¤±è´¥:', error.message);
    res.status(500).json({
      code: 500,
      message: 'ä»»åŠ¡æäº¤å¤±è´¥',
      error: error.message
    });
  }
});

/**
 * æäº¤çŒªç±»å‹ç»Ÿè®¡ä»»åŠ¡
 */
app.post('/api/hadoop/submit-pig-type-stats', (req, res) => {
  try {
    const HADOOP_HOME = process.env.HADOOP_HOME || '/opt/homebrew/opt/hadoop/libexec';
    const jobName = 'pig-type-statistics';
    const inputPath = '/pig-system/dashboard';  // ä¿®æ”¹ä¸ºå®é™…å­˜åœ¨çš„è·¯å¾„
    const outputPath = `/pig-system/output/type_stats_${Date.now()}`;  // è¾“å‡ºåˆ° output ç›®å½•

    console.log(`ğŸš€ [Hadoop Bridge] æ­£åœ¨æäº¤ MapReduce ä»»åŠ¡: ${jobName}`);

    const command = `
      export HADOOP_HOME=${HADOOP_HOME} && \
      JAR_PATH=$(ls ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar 2>/dev/null | head -n1) && \
      hadoop jar "$JAR_PATH" \
        -D mapreduce.job.name="${jobName}" \
        -input "${inputPath}" \
        -output "${outputPath}" \
        -mapper "cut -f2" \
        -reducer "sort | uniq -c"
    `.trim();

    const child = exec(command, {
      env: { ...process.env, HADOOP_HOME },
      maxBuffer: 1024 * 1024 * 10,
      shell: '/bin/bash'
    });

    console.log(`âœ… ä»»åŠ¡å·²æäº¤ï¼ŒPID: ${child.pid}`);

    res.json({
      code: 200,
      message: 'çŒªç±»å‹ç»Ÿè®¡ä»»åŠ¡å·²æäº¤',
      data: {
        pid: child.pid,
        jobName,
        submittedAt: new Date().toISOString()
      }
    });
  } catch (error) {
    console.error('âŒ æäº¤ä»»åŠ¡å¤±è´¥:', error.message);
    res.status(500).json({
      code: 500,
      message: 'ä»»åŠ¡æäº¤å¤±è´¥',
      error: error.message
    });
  }
});

// é”™è¯¯å¤„ç†
app.use((err, req, res, next) => {
  console.error('âŒ æœåŠ¡å™¨é”™è¯¯:', err.message);
  res.status(500).json({
    code: 500,
    message: 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯',
    error: process.env.NODE_ENV === 'development' ? err.message : 'æœåŠ¡å¼‚å¸¸'
  });
});

// å¯åŠ¨æœåŠ¡å™¨ï¼ˆå¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼‰
if (require.main === module) {
  const server = app.listen(PORT, '0.0.0.0', () => {
    console.log('ğŸš€ ========================================');
    console.log(`ğŸš€ Hadoop Bridge æœåŠ¡å·²å¯åŠ¨`);
    console.log(`ğŸš€ ç›‘å¬ç«¯å£: ${PORT}`);
    console.log(`ğŸš€ ç›‘å¬åœ°å€: 0.0.0.0 (æ‰€æœ‰ç½‘ç»œæ¥å£)`);
    console.log(`ğŸš€ HADOOP_HOME: ${process.env.HADOOP_HOME || 'æœªè®¾ç½®'}`);
    console.log('ğŸš€ ========================================');
  });

  server.on('error', (err) => {
    if (err.code === 'EADDRINUSE') {
      console.error(`âŒ ç«¯å£ ${PORT} è¢«å ç”¨`);
      process.exit(1);
    }
  });
}

module.exports = app;
