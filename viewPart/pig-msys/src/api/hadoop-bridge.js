/**
 * Hadoop MapReduce ä»»åŠ¡æäº¤æ¡¥æ¥æ¨¡å—
 * ç”¨äºä» Node.js è§¦å‘ Hadoop Streaming ä»»åŠ¡ï¼ˆfire-and-forgetï¼‰
 */
const { exec } = require('child_process');
const path = require('path');

/**
 * æäº¤ MapReduce ä»»åŠ¡ï¼ˆå¼‚æ­¥éé˜»å¡ï¼‰
 * @param {Object} options - ä»»åŠ¡é…ç½®é€‰é¡¹
 * @param {string} options.jobName - ä»»åŠ¡åç§°
 * @param {string} options.input - HDFS è¾“å…¥è·¯å¾„
 * @param {string} options.output - HDFS è¾“å‡ºè·¯å¾„
 * @param {string} options.mapper - Mapper è„šæœ¬è·¯å¾„
 * @param {string} options.reducer - Reducer è„šæœ¬è·¯å¾„
 * @returns {number} å­è¿›ç¨‹ PID
 */
const submitMapReduceJob = (options = {}) => {
  // ä»ç¯å¢ƒå˜é‡è·å– Hadoop è·¯å¾„
  const HADOOP_HOME = process.env.HADOOP_HOME || '/opt/homebrew/opt/hadoop/libexec';
  
  // æ„é€  hadoop-streaming jar è·¯å¾„ï¼ˆä½¿ç”¨é€šé…ç¬¦ï¼‰
  const streamingJarPattern = `${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-*.jar`;
  
  // é»˜è®¤å‚æ•°
  const jobName = options.jobName || 'pig-environment-analysis';
  const inputPath = options.input || '/user/hadoop/pig_data/environment';
  const outputPath = options.output || `/user/hadoop/pig_output/analysis_${Date.now()}`;
  const mapper = options.mapper || 'cat'; // é»˜è®¤ä½¿ç”¨ catï¼ˆç›´æ¥è¾“å‡ºï¼‰
  const reducer = options.reducer || 'wc -l'; // é»˜è®¤ä½¿ç”¨ wc ç»Ÿè®¡è¡Œæ•°
  
  // æ„é€ å®Œæ•´çš„ Hadoop Streaming å‘½ä»¤
  // æ³¨æ„ï¼šä½¿ç”¨ sh -c ä»¥æ”¯æŒé€šé…ç¬¦å±•å¼€
  const command = `
    export HADOOP_HOME=${HADOOP_HOME} && \
    JAR_PATH=$(ls ${streamingJarPattern} 2>/dev/null | head -n1) && \
    if [ -z "$JAR_PATH" ]; then
      echo "é”™è¯¯: æ‰¾ä¸åˆ° hadoop-streaming jar æ–‡ä»¶" >&2
      exit 1
    fi && \
    hadoop jar "$JAR_PATH" \\
      -D mapreduce.job.name="${jobName}" \\
      -input "${inputPath}" \\
      -output "${outputPath}" \\
      -mapper "${mapper}" \\
      -reducer "${reducer}"
  `.trim();

  console.log(`ğŸš€ [Hadoop Bridge] æ­£åœ¨æäº¤ MapReduce ä»»åŠ¡...`);
  console.log(`   - ä»»åŠ¡åç§°: ${jobName}`);
  console.log(`   - è¾“å…¥è·¯å¾„: ${inputPath}`);
  console.log(`   - è¾“å‡ºè·¯å¾„: ${outputPath}`);
  console.log(`   - Mapper: ${mapper}`);
  console.log(`   - Reducer: ${reducer}`);

  // å¼‚æ­¥æ‰§è¡Œå‘½ä»¤ï¼ˆfire-and-forgetï¼‰
  const child = exec(command, {
    env: { ...process.env, HADOOP_HOME },
    maxBuffer: 1024 * 1024 * 10, // 10MB buffer
    shell: '/bin/bash' // æ˜ç¡®ä½¿ç”¨ bash
  });

  const pid = child.pid;
  console.log(`âœ… [Hadoop Bridge] ä»»åŠ¡å·²æäº¤ï¼Œè¿›ç¨‹ PID: ${pid}`);

  // ç›‘å¬è¾“å‡ºï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
  child.stdout.on('data', (data) => {
    console.log(`[Hadoop STDOUT]: ${data.toString().trim()}`);
  });

  child.stderr.on('data', (data) => {
    console.error(`[Hadoop STDERR]: ${data.toString().trim()}`);
  });

  child.on('exit', (code, signal) => {
    if (code === 0) {
      console.log(`âœ… [Hadoop Bridge] ä»»åŠ¡æ‰§è¡Œå®Œæˆ (PID: ${pid})`);
    } else {
      console.error(`âŒ [Hadoop Bridge] ä»»åŠ¡æ‰§è¡Œå¤±è´¥ (PID: ${pid}, é€€å‡ºç : ${code}, ä¿¡å·: ${signal})`);
    }
  });

  child.on('error', (err) => {
    console.error(`âŒ [Hadoop Bridge] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ (PID: ${pid}):`, err.message);
  });

  // ç«‹å³è¿”å›è¿›ç¨‹ IDï¼ˆä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼‰
  return pid;
};

/**
 * é¢„å®šä¹‰çš„ä»»åŠ¡æ¨¡æ¿ï¼šç¯å¢ƒæ•°æ®åˆ†æ
 */
const submitEnvironmentAnalysisJob = () => {
  return submitMapReduceJob({
    jobName: 'pig-environment-analysis',
    input: '/user/hadoop/pig_data/environment',
    output: `/user/hadoop/pig_output/env_analysis_${Date.now()}`,
    mapper: 'cat', // å¯æ›¿æ¢ä¸ºè‡ªå®šä¹‰ mapper è„šæœ¬
    reducer: 'wc -l'
  });
};

/**
 * é¢„å®šä¹‰çš„ä»»åŠ¡æ¨¡æ¿ï¼šçŒªç±»å‹ç»Ÿè®¡
 */
const submitPigTypeStatsJob = () => {
  return submitMapReduceJob({
    jobName: 'pig-type-statistics',
    input: '/user/hadoop/pig_data/pig_info',
    output: `/user/hadoop/pig_output/type_stats_${Date.now()}`,
    mapper: 'cut -f2', // æå–ç¬¬äºŒåˆ—ï¼ˆå‡è®¾æ˜¯çŒªç±»å‹ï¼‰
    reducer: 'sort | uniq -c'
  });
};

module.exports = {
  submitMapReduceJob,
  submitEnvironmentAnalysisJob,
  submitPigTypeStatsJob
};
