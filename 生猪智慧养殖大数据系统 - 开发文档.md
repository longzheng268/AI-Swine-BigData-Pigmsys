 生猪智慧养殖大数据系统 - 开发文档

 项目概述



基于 Hadoop + Spring Boot + Vue 3 的生猪智慧养殖大数据系统，集成环境监测、数据分析、智能预测等功能。



技术栈



 后端

框架: Spring Boot 2.6.7

语言: Java 1.8

ORM:MyBatis 2.2.2

数据库: MySQL + Druid 连接池

安全: Spring Security + JWT

大数据: Hadoop 2.10.2 (MapReduce)

工具: Lombok、PageHelper、Apache POI、FastJSON、Hutool



 前端

\- 框架: Vue 3.3.4

\- 路由: Vue Router 4.2.5

\- 状态管理: Vuex 4.1.0

\- UI组件: Element Plus 2.4.2

\-图表: ECharts 5.3.2

HTTP: Axios 0.27.2



 Python服务

框架: Flask 2.3.0

机器学习: scikit-learn 1.3.0

数值计算: NumPy 1.24.3



项目结构



\```

pig-sys-master/

├── javaPart/pigms/      # Spring Boot 后端

│  ├── src/main/java/com/zhu/

│  │  ├── controller/    # 控制器层 (12个)

│  │  ├── service/     # 业务逻辑层 (11个)

│  │  ├── mapper/      # 数据访问层 (11个)

│  │  ├── pojo/       # 实体类 (11个)

│  │  ├── config/      # 配置类

│  │  ├── filter/      # 过滤器 (JWT、CORS)

│  │  ├── utils/      # 工具类

│  │  └── hadoop/      # Hadoop MapReduce

│  └── src/main/resources/

│    ├── application.yml  # 配置文件

│    └── mybatis/mapper/  # MyBatis XML映射

├── viewPart/pig-msys/    # Vue 3 前端

│  └── src/

│    ├── views/      # 页面组件 (14个)

│    ├── components/    # 公共组件

│    ├── api/       # API接口封装

│    ├── router/      # 路由配置

│    └── utils/      # 工具函数

├── pythonService/      # Python 预测服务

│  ├── app.py        # Flask 应用

│  └── requirements.txt   # Python依赖

└── mysql/          # 数据库脚本

  ├── pigms.sql      # 主数据库脚本

  └── pigms_extension.sql # 扩展脚本

\```



环境要求



 JDK: 1.8+

Maven: 3.9+

Node.js: 16

Python: 3.12

MySQL: 8.0

Hadoop: 2.16



 快速开始



1. 数据库初始化



bash

* *创建数据库*

mysql -u root -p

CREATE DATABASE pigms CHARACTER SET utf8 COLLATE utf8_general_ci;



*导入数据*

mysql -u root -p pigms < mysql/pigms.sql

mysql -u root -p pigms < mysql/pigms_extension.sql

\```



2. 后端配置



编辑 `javaPart/pigms/src/main/resources/application.yml`:



yaml

spring:

 datasource:

  url: jdbc:mysql://localhost:3306/pigms?useUnicode=true&characterEncoding=utf-8

  username: root

  password: 你的密码



hadoop:

 namenode: hdfs://192.168.100.10:9000

 resourcemanager: 192.168.100.10:8032



python:

 service:

  url: http://localhost:5000

\```



启动后端:



cd javaPart/pigms

mvn spring-boot:run



\```



3. Python服务启动

cd pythonService

pip install -r requirements.txt

python app.py





4. 前端启动



cd viewPart/pig-msys

npm install

npm run serve





访问: http://localhost:8081



 核心功能模块



1. 用户认证与权限

JWT Token 认证

基于角色的权限控制 (ADMIN/USER/RESEARCHER)

\登录/注册/登出



2. 生猪信息管理

\- 生猪信息 CRUD

\- 猪类型管理

\- 生产周期分析 (出生/买入/交易)



3. 环境监测

\- 环境数据采集 (温度/湿度/空气质量等)

\- 异常数据预警

\- 历史数据查询



4. 数据分析

\- Hadoop MapReduce 数据分析

\- 数据可视化 (ECharts)

\- 可视化大屏展示



5. 智能预测

\- 生长预测

\- 环境质量评价

\- 疾病风险预测



6. 数据上传

\- Excel 文件上传

\- 批量数据导入

\- 上传记录管理



7. 系统管理

\- 用户管理

\- 操作日志

\- 模型管理



主要API接口



认证相关



POST /auth/login      # 登录

GET  /auth/validate    # 验证Token

POST /auth/logout     # 登出

POST /user/register    # 注册

\```



 生猪管理



GET  /pig/getTypeSum        # 获取猪类型统计

GET  /pigInfo/list         # 获取生猪列表

POST /pigInfo/list/search/{page}/{size}  # 分页查询

POST /addPig            # 新增生猪

GET  /pigInfo/{id}         # 获取详情

\```



 环境监测



POST /environment/add       # 新增环境数据

GET  /environment/list       # 获取列表

GET  /environment/latest      # 最新数据

GET  /environment/abnormal/count  # 异常数据统计

\```



数据分析



GET  /api/dashboard/data      # 仪表盘数据

GET  /api/dashboard/overview    # 概览统计

GET  /api/hadoop/analyze/pig    # 生猪数据分析

GET  /api/hadoop/analyze/environment  # 环境数据分析

\```



 预测分析



POST /prediction/predict      # 执行预测

GET  /prediction/models      # 获取模型列表

GET  /prediction/records      # 预测记录



 Hadoop操作

GET  /api/hadoop/status      # 集群状态

POST /api/hadoop/upload      # 上传文件到HDFS

GET  /api/hadoop/list       # 文件列表

POST /api/hadoop/analyze/pig    # MapReduce分析





 权限说明



| 角色 | 权限 |

|------|------|

| ADMIN (管理员) | 所有功能 |

| USER (普通用户) | 查看数据、个人信息管理 |

| RESEARCHER (科研人员) | 数据分析、预测分析 |



 数据库表结构



核心表

\- `tb_login` - 用户登录表

\- `tb_user` - 用户信息表

\- `tb_role` - 角色表

\- `tb_pig` - 生猪信息表

\- `tb_pigtype` - 猪类型表

\- `tb_environment_data` - 环境监测数据表

\- `tb_operation_log` - 操作日志表

\- `tb_model` - 模型表

\- `tb_prediction_record` - 预测记录表

\- `tb_upload_record` - 上传记录表



 开发规范



 后端

\- Controller 层负责接口定义和参数验证

\- Service 层处理业务逻辑

\- Mapper 层负责数据库操作

\- 统一使用 RESTful API 风格

\- 返回格式: `{code: 200, message: "success", data: {...}}`



 前端

\- 使用 Vue 3 Composition API

\- 组件化开发

\- API 统一封装在 `src/api/` 目录

\- 路由配置在 `src/router/index.js`

\- 统一使用 Element Plus 组件



代码规范

Java: 驼峰命名

JavaScript: 驼峰命名

数据库: 下划线命名



 常见问题

1. 跨域问题

已在 `SecurityConfig.java` 中配置 CORS，允许所有来源访问。



2. JWT Token 过期

前端会自动跳转到登录页，重新登录获取新Token。



3. Hadoop 连接失败

检查 `application.yml` 中的 Hadoop 配置，确保集群地址正确。



4. Python 服务无法访问

确保 Python 服务运行在 5000 端口，或修改配置中的服务地址。











 默认账号



\- 用户名: `admin`

\- 密码: `123456789`

\- 角色: 管理员





